---
title: "Data Dictionary: *Puget Sound Shellfish* DB"
author: "Are Strom"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: spacelab
    toc: true
    toc_float: true
    toc_depth: 4
---

![](aerial.png){ width=50% }


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction
The proposed WDFW *Puget Sound Shellfish* database, described below, is intended to house data needed by the Puget Sound Shellfish Unit to generate harvest estimates and manage fisheries for clams, oysters, crab, and shrimp in the Puget Sound Basin. This is an amended version of the current *Shellfish* database model that currently houses data for the Intertidal Shellfish Unit. Changes were made to generalize the current model, and to allow for more flexible storage of a wider range of data. 

#### Rationale for a new database  

Development of the new database was motivated largely by the desire to: 

* Simplify handling of spatial data under one core *location* table.
* Generalize storage of geographic features into separate tables for lines, points, and polygons.
* Store GIS data in a more approachable *latitude-longitude* format. 
* Generalize the current data model to more flexibly store additional data.

The general aim of the data model is to provide a *one-database* solution for all survey and ancillary data needed to generate harvest estimates. Safety is enhanced by automated daily backups. The *shellfish* database enables biologist to keep all their survey, GIS, and management argreement data in one place. It avoids the need to piece together data from disparate sources during the analysis phase, and allows the current set of programming code used to generate harvest estimates to draw all needed datasets from one comprehensive, and properly linked location.  

To facilitate data sharing, the *Shellfish* database has been designed using technology that make it easier to integrate and disseminate data. For example, by using universally unique IDs (UUIDs) for database keys, the need for central coordination in assigning database keys has been eliminated. By hosting the database remotely in the cloud, there is greater flexibility in engineering secure solutions to allow outside entities to easily access and use the data.

#### Spatial features

The *Puget Sound Shellfish* database allows for storing spatial data types. With proper permissions, users can connect directly to the database server using GIS tools such as [QGIS](https://qgis.org/en/site/) to enter data, create, edit, or delete spatial features. When using [QGIS](https://qgis.org/en/site/), no additional server or middleware is required. The primary change from the existing *Shellfish* database is that the new model stores all GIS data as a set of separate tables for points, lines, and polygons, organized under the central *location* table. Also, instead of storing GIS data in a projected geometry format (EPSG:2927), points, lines, and polygons are now stored in the somewhat more approachable geographical, *latitude-longitude* (EPSG:4326), format. 

#### Overview diagram (table names only)

![Table names only diagram](./PSSFTitleDiagram.pdf){width=100% height=1600}

#### Notes on table descriptions

The sections below document table and field definitions for the *Puget Sound Shellfish* database. Table descriptions are arranged in descending hierarchical order starting with top-level header data for individual surveys, and ending with tables that store data for individual species encounters. Each section describes one or more *data* tables that share a common theme, or *level* in the hierarchy. Each section is preceeded by a diagram of table relationships. 

Within each section, finer-grained descriptions of columns are provided for each table. For cases where columns link to a set of look-up table values, the relevant look-up tables are displayed directly below the *data* table descriptions. If a look-up table links to more than one *data* table, then the details of that table will only be displayed once...the first time it is mentioned.

The final four columns of each *data* table include:

* *created_datetime*: timestamptz(6)   
* *created_by*: varchar(16)  
* *modified_datetime*: timestamptz(6)  
* *modified_by*: varchar(16)  

These columns document the *time* (with timezone) that each record was created or modified, and the *individual* or *application* that created or modified the record. For brevity these columns will not be included in the *data* table descriptions below, but should be assumed to be present. The timezone is always converted from local and recorded in the database in UTC format (Coordinated Universal Time).

Look-up tables can be identified by the underscore *lut* suffix in the table name. For example, *data_source_lut*. The final two columns of each *look-up* table include:  

* *obsolete_flag*: boolean  
* *obsolete_datetime* timestamptz(6)  

These columns document if a field has been *retired* or deemed *obsolete*, along with the date when the field was retired. For brevity these columns will not be included in the *look-up* table descriptions below, but should be assumed to be present.  

Values recorded in each look-up table are displayed directly below the field definitions. For look-up tables with a large number of values, only a selection of values will be displayed. Some look-up tables such as the *species_lut* may be linked to more than one *data* table. In these cases the look-up table will only be displayed the first time it is mentioned. 

#### Disclaimer

The *Puget Sound Shellfish* database is a work in progress and is subject to change. Please check with the *WDFW Puget Sound Shellfish Unit* in Port Townsend, WA for the latest updates. This site will likely remain static and not reflect any changes implemented after February 2023. 

```{r message = FALSE, warning = FALSE, echo = FALSE}
library(DBI)
library(RPostgres)
library(dplyr)
library(knitr)
library(stringi)
library(tibble)
library(DiagrammeR)
library(datamodelr)
library(DiagrammeRsvg)
library(rsvg)

# Specify db and host
db_source = "ps_shellfish"
host_source = "pg_host_local"

#=========================================================
source('C:/RSourceFiles/GenericFunctions.R')
#=========================================================

# Function to get user for database
pg_user <- function(user_label) {
  Sys.getenv(user_label)
}
# Function to get pw for database
pg_pw <- function(pwd_label) {
  Sys.getenv(pwd_label)
}
# Function to get pw for database
pg_host <- function(host_label) {
  Sys.getenv(host_label)
}
# Function to connect to postgres
pg_con = function(hostname, dbname, port = '5432') {
  con <- dbConnect(
    RPostgres::Postgres(),
    host = pg_host(hostname),
    dbname = dbname,
    user = pg_user("pg_user"),
    password = pg_pw("pg_pwd_local"),
    port = port)
  con
}

# Function to generate dataframe of tables and row counts in database
pg_table_metadata = function(table_name, database_name, hostname) {
  db_con = pg_con(hostname = host_source, dbname = db_source)
  db_meta = 
    dbGetQuery(db_con, 
               paste0("select * from public_tables_metadata ", 
                      "where table_name = '", table_name, "'"))
  dbDisconnect(db_con)
  # Process
  dat = as_tibble(db_meta)
  dat = dat %>% 
    mutate(is_nullable = stri_trans_totitle(is_nullable)) %>% 
    mutate(data_type_two = if_else(udt_name == "varchar", 
                                   paste0(udt_name, " (", character_maximum_length, ")"), udt_name)) %>% 
    mutate(data_type_two = if_else(udt_name == "timestamptz", 
                                   paste0(udt_name, " (", datetime_precision, ")"), data_type_two)) %>%
    mutate(data_type_two = if_else(udt_name == "numeric", 
                                   paste0(udt_name, " (", numeric_precision, ", ", numeric_scale,  ")"), 
                                   data_type_two)) %>% 
    mutate(Key = if_else(ordinal_position == 1 & data_type == "uuid" & is_nullable == "No",
                         "Primary", NA_character_)) %>% 
    mutate(name_suffix = substr(column_name, nchar(column_name) - 1, nchar(column_name))) %>% 
    mutate(Key = if_else(ordinal_position > 1 & data_type_two == "uuid" & name_suffix == "id",
                         "Foreign", Key)) %>% 
    arrange(ordinal_position) %>% 
    mutate(Key = if_else(is.na(Key), "", Key)) %>% 
    mutate(column_comment = if_else(is.na(column_comment), "", column_comment)) %>%
    filter(!column_name %in% c("created_datetime", "created_by",
                               "modified_datetime", "modified_by", 
                               "obsolete_flag", "obsolete_datetime")) %>% 
    select(ColumnName = column_name, DataType = data_type_two, IsNullable = is_nullable, Key,
           Description = column_comment)  
  # Rename  
  names(dat) = c("Column name", "Data type", "Nullable?", "Key", "Description")
  dat
}

# Function to get only comment on table, not columns
pg_table_comment = function(table_name, db_source, host_source) {
  db_con = pg_con(hostname = host_source, dbname = db_source)
  table_comment = 
    dbGetQuery(db_con, 
               paste0("SELECT pg_catalog.obj_description('", 
                      table_name, "'::regclass, 'pg_class')"))
  dbDisconnect(db_con)
  as.character(table_comment)
}

#======================================================================
# Function to get part of a string separated by ....
get_text_item <- function(x, item = 2, sep= " ") {
  get_list_item <- function(x, item = 2) {
    if(is.na(x[item])) {
      x = NA
    } else {
      x = x[item]
    }
    x
  }
  # Create list with all text items
  nms = strsplit(x, sep)
  # Extract the text at item position from the list
  nm = unlist(lapply(nms, get_list_item, item))
  nm
}

```

# Location  

### Location focused diagram

![Location focused diagram](./PSSFLocationDiagram.pdf){width=100% height=1200}

### Location
##### *`r pg_table_comment("location", db_source, host_source)`*

```{r message = FALSE, warning = FALSE, echo = FALSE, include = FALSE}
meta_data = pg_table_metadata("location", db_source, host_source)
```

```{r echo = FALSE}
knitr::kable(meta_data)
```

#### Location Type LUT  
##### *`r pg_table_comment("location_type_lut", db_source, host_source)`*

```{r message = FALSE, warning = FALSE, echo = FALSE, include = FALSE}
db_source_con = pg_con(hostname = host_source, dbname = db_source)
dat = dbReadTable(db_source_con, 'location_type_lut')
dbDisconnect(db_source_con)

dat = as_tibble(dat)
dat = dat %>%
  filter(!obsolete_flag == TRUE)
```

```{r message = FALSE, warning = FALSE, echo = FALSE, include = FALSE}
meta_data = pg_table_metadata("location_type_lut", db_source, host_source)
```

```{r echo = FALSE}
knitr::kable(meta_data)
```
```{r echo = FALSE}
kable(dat[, 1:2])
```


